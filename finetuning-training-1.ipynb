{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10211396,"sourceType":"datasetVersion","datasetId":6311256}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y\n!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T01:20:21.587387Z","iopub.execute_input":"2024-12-16T01:20:21.588127Z","iopub.status.idle":"2024-12-16T01:23:25.456096Z","shell.execute_reply.started":"2024-12-16T01:20:21.588074Z","shell.execute_reply":"2024-12-16T01:23:25.454913Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:44:31.085913Z","iopub.execute_input":"2024-12-16T02:44:31.086296Z","iopub.status.idle":"2024-12-16T02:44:31.092422Z","shell.execute_reply.started":"2024-12-16T02:44:31.086265Z","shell.execute_reply":"2024-12-16T02:44:31.091534Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from datasets import load_from_disk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:44:36.953774Z","iopub.execute_input":"2024-12-16T02:44:36.954148Z","iopub.status.idle":"2024-12-16T02:44:36.959915Z","shell.execute_reply.started":"2024-12-16T02:44:36.954114Z","shell.execute_reply":"2024-12-16T02:44:36.959027Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"dataset_path =\"/kaggle/input/user-metadata\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:44:43.892601Z","iopub.execute_input":"2024-12-16T02:44:43.893190Z","iopub.status.idle":"2024-12-16T02:44:43.898576Z","shell.execute_reply.started":"2024-12-16T02:44:43.893154Z","shell.execute_reply":"2024-12-16T02:44:43.897602Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"hf_token = \"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\"\n# Load the model and tokenizer from the pre-trained FastLanguageModel\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"meta-llama/Llama-3.1-8B-Instruct\",\n    max_seq_length = 2048,\n    dtype = None,\n    token = hf_token,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:44:49.657193Z","iopub.execute_input":"2024-12-16T02:44:49.657760Z","iopub.status.idle":"2024-12-16T02:46:05.918082Z","shell.execute_reply.started":"2024-12-16T02:44:49.657729Z","shell.execute_reply":"2024-12-16T02:46:05.917339Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2024.12.4: Fast Llama patching. Transformers:4.46.3.\n   \\\\   /|    GPU: Tesla P100-PCIE-16GB. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 6.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72dba5e02392476f8d75bb27104467f9"}},"metadata":{}},{"name":"stdout","text":"meta-llama/Llama-3.1-8B-Instruct does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Apply LoRA (Low-Rank Adaptation) adapters to the model for parameter-efficient fine-tuning\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:47:04.030255Z","iopub.execute_input":"2024-12-16T02:47:04.030595Z","iopub.status.idle":"2024-12-16T02:47:10.471741Z","shell.execute_reply.started":"2024-12-16T02:47:04.030562Z","shell.execute_reply":"2024-12-16T02:47:10.470797Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"meta_dataset = load_from_disk(dataset_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:47:11.148876Z","iopub.execute_input":"2024-12-16T02:47:11.149309Z","iopub.status.idle":"2024-12-16T02:47:11.763334Z","shell.execute_reply.started":"2024-12-16T02:47:11.149274Z","shell.execute_reply":"2024-12-16T02:47:11.762550Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = meta_dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = 2048,\n    dataset_num_proc = 2,\n    packing = False,\n    args = TrainingArguments(\n        per_device_train_batch_size = 4,\n        gradient_accumulation_steps = 8,\n        warmup_steps = 5,\n        max_steps = 100,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        output_dir = \"/kaggle/working\",\n    ),\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:49:06.627622Z","iopub.execute_input":"2024-12-16T02:49:06.628281Z","iopub.status.idle":"2024-12-16T02:49:06.696756Z","shell.execute_reply.started":"2024-12-16T02:49:06.628234Z","shell.execute_reply":"2024-12-16T02:49:06.695842Z"}},"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:49:14.577014Z","iopub.execute_input":"2024-12-16T02:49:14.577886Z","iopub.status.idle":"2024-12-16T04:20:03.290893Z","shell.execute_reply.started":"2024-12-16T02:49:14.577848Z","shell.execute_reply":"2024-12-16T04:20:03.290033Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 700,000 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 8\n\\        /    Total batch size = 32 | Total steps = 100\n \"-____-\"     Number of trainable parameters = 41,943,040\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 1:29:48, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>4.015800</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>4.012600</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3.946100</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3.575200</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>3.244500</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.908700</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>2.659600</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>2.404700</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>2.058300</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.718500</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.491000</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.331200</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.058300</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.068300</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.916200</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.970000</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.929100</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.980600</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.009100</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.839900</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.929000</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>1.114000</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.896500</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.953000</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>1.048500</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.942500</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.824600</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.839400</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.827000</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.030100</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.791100</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.926500</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.863100</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.861000</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.942500</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.893100</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.852500</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.918300</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.830700</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.778900</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.855100</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.887100</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.787200</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.907600</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.785400</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.766400</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.848100</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.782300</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.785600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.815700</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>0.844500</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>0.859400</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>0.677400</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.915100</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.803200</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.840700</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>0.844600</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.774000</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.795900</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.843100</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>0.793800</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>0.893500</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>0.759200</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>0.794200</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.837200</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>0.823700</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>0.857800</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>0.798200</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>0.776900</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.854800</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>0.901700</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>0.810000</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>0.784900</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>0.841600</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.827700</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>0.801000</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>0.909200</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>0.824300</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>0.866500</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.731100</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>0.697800</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>0.756700</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>0.842600</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>0.794000</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>0.784300</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>0.842500</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>0.770100</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>0.803500</td>\n    </tr>\n    <tr>\n      <td>89</td>\n      <td>0.827200</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.870000</td>\n    </tr>\n    <tr>\n      <td>91</td>\n      <td>0.816500</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>0.854500</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>0.791900</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>0.724600</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>0.798200</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>0.765900</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>0.820800</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>0.929400</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>0.758100</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.753800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-675faa72-1c53f8a35f5049c33f0e543f;8f3f160e-47f7-4d48-b01a-86e674f25b38)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"model.save_pretrained(\"kaggle/working/finetuned_model_meta\")\ntokenizer.save_pretrained(\"kaggle/working/finetuned_model_meta\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T04:21:52.495164Z","iopub.execute_input":"2024-12-16T04:21:52.495618Z","iopub.status.idle":"2024-12-16T04:21:53.086181Z","shell.execute_reply.started":"2024-12-16T04:21:52.495577Z","shell.execute_reply":"2024-12-16T04:21:53.085083Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-675faae0-458fd5b9499017392fb9a700;9f0e1cd4-70ba-4c0e-9d94-8b649ea70751)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"('kaggle/working/finetuned_model_meta/tokenizer_config.json',\n 'kaggle/working/finetuned_model_meta/special_tokens_map.json',\n 'kaggle/working/finetuned_model_meta/tokenizer.json')"},"metadata":{}}],"execution_count":24}]}