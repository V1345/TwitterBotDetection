{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f80692f-cd02-49af-bbf4-1bcb18b59daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home1/ms74647/.local/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home1/ms74647/.local/lib/python3.12/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /spack/conda/envs/ood-jupyterlab-4.2/lib/python3.12/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /spack/conda/envs/ood-jupyterlab-4.2/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home1/ms74647/.local/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /spack/conda/envs/ood-jupyterlab-4.2/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6c6ea9c-cebd-40a1-9778-61a73c39221b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /home1/ms74647/.local/lib/python3.12/site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home1/ms74647/.local/lib/python3.12/site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install pyspark\n",
    "# pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec489838-4280-4c7b-8dbd-d7de07684013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys; os.environ[\"PYSPARK_PYTHON\"] = sys.executable; os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python3\"; from pyspark.sql.functions import col; from pyspark.sql import SparkSession; import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be156b2-aa64-4869-9e8b-638fd2831662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmod\n",
    "await lmod.purge(force=True)\n",
    "await lmod.load('jdk/17.0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "887fad87-cbc0-43e3-944f-45edec5c5a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/23 23:07:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b11-12.hpc.usc.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Process Large JSON</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f13439af650>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"Process Large JSON\")\\\n",
    "    .config(\"spark.executor.memory\", \"96g\")\\\n",
    "    .config(\"spark.driver.memory\", \"64g\")\\\n",
    "    .getOrCreate()\n",
    "file_path = \"/project/swabhas_1457/Section_20243_30249_26/Data/Tweets/tweet_1.json\"\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43cf3289-a853-44c8-b298-b33fc492414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Set the Python version for Spark driver and worker\n",
    "# python_path = sys.executable  # Get the path of the currently used Python (which should be 3.12.7)\n",
    "\n",
    "# # Set environment variables to ensure same Python version in both driver and workers\n",
    "# os.environ[\"PYSPARK_PYTHON\"] = python_path\n",
    "# os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python3\"  # Ensure the driver uses Python 3.x\n",
    "\n",
    "# # Verify the environment variables are set correctly\n",
    "# print(f\"PYSPARK_PYTHON is set to: {os.environ['PYSPARK_PYTHON']}\")\n",
    "# print(f\"PYSPARK_DRIVER_PYTHON is set to: {os.environ['PYSPARK_DRIVER_PYTHON']}\")\n",
    "\n",
    "# print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f62190e-b7c6-4c4d-95f6-6ab3531578d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 23:07:18,492 - INFO - Reading labels and split data...\n",
      "2024-11-23 23:07:21,921 - INFO - Loaded labels data with 1000000 rows and split data with 1000000 rows.\n",
      "2024-11-23 23:07:21,922 - INFO - Processing tweet file: /project/swabhas_1457/Section_20243_30249_26/Data/Tweets/tweet_7.json\n",
      "2024-11-23 23:11:16,639 - INFO - Loaded tweets data with 6726905 rows from /project/swabhas_1457/Section_20243_30249_26/Data/Tweets/tweet_7.json.\n",
      "2024-11-23 23:12:53,789 - INFO - Joined dataset contains 6726905 rows.          \n",
      "2024-11-23 23:12:53,789 - INFO - Splitting data into train, val, and test...\n",
      "2024-11-23 23:18:02,809 - INFO - Train set: 5273729 rows, Validation set: 940312 rows, Test set: 478005 rows.\n",
      "2024-11-23 23:18:02,810 - INFO - Selecting up to 5 random tweets per user in the training data...\n",
      "2024-11-23 23:20:55,767 - INFO - Sampled train set contains 997121 rows.        \n",
      "2024-11-23 23:20:55,768 - INFO - Saving splits to Parquet files...\n",
      "2024-11-23 23:20:55,768 - INFO - /project/swabhas_1457/Section_20243_30249_26/Data/Tweet_SingleFile_Split/tweets_train_0.parquet exists, appending data...\n",
      "2024-11-23 23:23:45,727 - INFO - /project/swabhas_1457/Section_20243_30249_26/Data/Tweet_SingleFile_Split/tweets_val_0.parquet exists, appending data...\n",
      "2024-11-23 23:25:28,414 - INFO - /project/swabhas_1457/Section_20243_30249_26/Data/Tweet_SingleFile_Split/tweets_test_0.parquet exists, appending data...\n",
      "2024-11-23 23:27:09,550 - INFO - Appended train data to: /project/swabhas_1457/Section_20243_30249_26/Data/Tweet_SingleFile_Split/tweets_train_0.parquet\n",
      "2024-11-23 23:27:09,550 - INFO - Appended val data to: /project/swabhas_1457/Section_20243_30249_26/Data/Tweet_SingleFile_Split/tweets_val_0.parquet\n",
      "2024-11-23 23:27:09,550 - INFO - Appended test data to: /project/swabhas_1457/Section_20243_30249_26/Data/Tweet_SingleFile_Split/tweets_test_0.parquet\n",
      "2024-11-23 23:27:09,551 - INFO - Processing tweet file: /project/swabhas_1457/Section_20243_30249_26/Data/Tweets/tweet_8.json\n",
      "2024-11-23 23:30:32,917 - INFO - Loaded tweets data with 5569490 rows from /project/swabhas_1457/Section_20243_30249_26/Data/Tweets/tweet_8.json.\n",
      "2024-11-23 23:31:53,878 - INFO - Joined dataset contains 5569490 rows.          \n",
      "2024-11-23 23:31:53,879 - INFO - Splitting data into train, val, and test...\n",
      "2024-11-23 23:36:14,802 - INFO - Train set: 4399837 rows, Validation set: 769971 rows, Test set: 375230 rows.\n",
      "2024-11-23 23:36:14,802 - INFO - Selecting up to 5 random tweets per user in the training data...\n",
      "2024-11-23 23:38:43,146 - INFO - Sampled train set contains 971376 rows.        \n",
      "2024-11-23 23:38:43,147 - INFO - Saving splits to Parquet files...\n",
      "2024-11-23 23:38:43,148 - INFO - /project/swabhas_1457/Section_20243_30249_26/Data/Tweet_SingleFile_Split/tweets_train_0.parquet exists, appending data...\n",
      "2024-11-23 23:41:14,508 - INFO - /project/swabhas_1457/Section_20243_30249_26/Data/Tweet_SingleFile_Split/tweets_val_0.parquet exists, appending data...\n",
      "2024-11-23 23:42:42,365 - INFO - /project/swabhas_1457/Section_20243_30249_26/Data/Tweet_SingleFile_Split/tweets_test_0.parquet exists, appending data...\n",
      "2024-11-23 23:44:09,181 - INFO - Appended train data to: /project/swabhas_1457/Section_20243_30249_26/Data/Tweet_SingleFile_Split/tweets_train_0.parquet\n",
      "2024-11-23 23:44:09,182 - INFO - Appended val data to: /project/swabhas_1457/Section_20243_30249_26/Data/Tweet_SingleFile_Split/tweets_val_0.parquet\n",
      "2024-11-23 23:44:09,182 - INFO - Appended test data to: /project/swabhas_1457/Section_20243_30249_26/Data/Tweet_SingleFile_Split/tweets_test_0.parquet\n",
      "2024-11-23 23:44:09,182 - INFO - Validating saved files...\n",
      "2024-11-23 23:44:11,545 - INFO - Train data: 8162939 rows.                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+-----+\n",
      "|             user_id|                text|label|split|\n",
      "+--------------------+--------------------+-----+-----+\n",
      "|         u1000012406|@Lesbian_Moses @_...|human|train|\n",
      "|u1000087449933107200|@PWilliams101 @se...|human|train|\n",
      "|u1000130044897918977|RT @cricketcrocke...|human|train|\n",
      "|u1000130044897918977|RT @GlennsTheorem...|human|train|\n",
      "|u1000130044897918977|@ike_onwuka Thank...|human|train|\n",
      "+--------------------+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 23:44:13,774 - INFO - Validation data: 9981966 rows.                 \n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+-----+\n",
      "|             user_id|                text|label|split|\n",
      "+--------------------+--------------------+-----+-----+\n",
      "|u1000709400435216389|RT @AmazingInnova...|human|  val|\n",
      "|u1000709400435216389|RT @scienceClub01...|human|  val|\n",
      "|u1000709400435216389|RT @AmazingNature...|human|  val|\n",
      "|u1000709400435216389|RT @amazing_physi...|human|  val|\n",
      "|u1000709400435216389|RT @MachinePix: T...|human|  val|\n",
      "+--------------------+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 23:44:15,621 - INFO - Test data: 5954349 rows.                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+-----+\n",
      "|   user_id|                text|label|split|\n",
      "+----------+--------------------+-----+-----+\n",
      "|u100301671|@meganakpeters @2...|human| test|\n",
      "|u100301671|@meganakpeters Th...|human| test|\n",
      "|u100301671|@Fitzy_Red @chipn...|human| test|\n",
      "|u100301671|RT @KordingLab: A...|human| test|\n",
      "|u100301671|  Working on teyf :)|human| test|\n",
      "+----------+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Define paths\n",
    "tweet_files_base_path = \"/project/swabhas_1457/Section_20243_30249_26/Data/Tweets/tweet_\"\n",
    "label_file_path = \"/project/swabhas_1457/Section_20243_30249_26/Data/label.csv\"\n",
    "split_file_path = \"/project/swabhas_1457/Section_20243_30249_26/Data/split.csv\"\n",
    "\n",
    "output_dir = \"/project/swabhas_1457/Section_20243_30249_26/Data/Tweet_SingleFile_Split/\"\n",
    "train_file = os.path.join(output_dir, \"tweets_train_0.parquet\")\n",
    "val_file = os.path.join(output_dir, \"tweets_val_0.parquet\")\n",
    "test_file = os.path.join(output_dir, \"tweets_test_0.parquet\")\n",
    "\n",
    "# Step 1: Read labels and split data\n",
    "logger.info(\"Reading labels and split data...\")\n",
    "labels_df = spark.read.csv(label_file_path, header=True).selectExpr(\"id as user_id\", \"label\")\n",
    "split_df = spark.read.csv(split_file_path, header=True).selectExpr(\"id as split_id\", \"split\")\n",
    "logger.info(f\"Loaded labels data with {labels_df.count()} rows and split data with {split_df.count()} rows.\")\n",
    "\n",
    "# Check if the files exist and decide whether to append or overwrite\n",
    "def save_to_parquet(df, file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        logger.info(f\"{file_path} exists, appending data...\")\n",
    "        df.write.mode(\"append\").parquet(file_path)\n",
    "    else:\n",
    "        logger.info(f\"{file_path} does not exist, creating a new file...\")\n",
    "        df.write.mode(\"overwrite\").parquet(file_path)\n",
    "\n",
    "# Step 2: Process each tweet file (from tweet_0.json to tweet_8.json)\n",
    "for i in range(7,9):  # Assuming files are named tweet_0.json, tweet_1.json, ..., tweet_8.json\n",
    "    tweet_file_path = f\"{tweet_files_base_path}{i}.json\"\n",
    "    \n",
    "    if os.path.exists(tweet_file_path):\n",
    "        logger.info(f\"Processing tweet file: {tweet_file_path}\")\n",
    "        \n",
    "        # Step 2.1: Read tweets data\n",
    "        tweets_df = spark.read.option(\"multiline\", \"true\").json(tweet_file_path).select(\"author_id\", \"text\", \"lang\").dropna()\n",
    "        tweets_df = tweets_df.filter(col(\"lang\") == \"en\").selectExpr(\"concat('u', author_id) as user_id\", \"text\")\n",
    "        logger.info(f\"Loaded tweets data with {tweets_df.count()} rows from {tweet_file_path}.\")\n",
    "        \n",
    "        # Step 2.2: Join with labels and split data\n",
    "        tweets_labels_df = tweets_df.join(labels_df, on=\"user_id\", how=\"inner\") \\\n",
    "                                    .join(split_df, tweets_df.user_id == split_df.split_id, how=\"inner\") \\\n",
    "                                    .drop(\"split_id\")\n",
    "        logger.info(f\"Joined dataset contains {tweets_labels_df.count()} rows.\")\n",
    "\n",
    "        # Step 2.3: Split data into train, val, and test\n",
    "        logger.info(\"Splitting data into train, val, and test...\")\n",
    "        train_df = tweets_labels_df.filter(col(\"split\") == \"train\").dropDuplicates()\n",
    "        val_df = tweets_labels_df.filter(col(\"split\") == \"val\").dropDuplicates()\n",
    "        test_df = tweets_labels_df.filter(col(\"split\") == \"test\").dropDuplicates()\n",
    "        logger.info(f\"Train set: {train_df.count()} rows, Validation set: {val_df.count()} rows, Test set: {test_df.count()} rows.\")\n",
    "        \n",
    "        # Step 2.4: Select 5 random tweets per user in training data\n",
    "        logger.info(\"Selecting up to 5 random tweets per user in the training data...\")\n",
    "        train_sampled_df = train_df.groupBy(\"user_id\").applyInPandas(\n",
    "            lambda pdf: pdf.sample(n=min(len(pdf), 5), random_state=42),\n",
    "            schema=train_df.schema\n",
    "        )\n",
    "        logger.info(f\"Sampled train set contains {train_sampled_df.count()} rows.\")\n",
    "        \n",
    "        # Step 2.5: Save data to Parquet\n",
    "        logger.info(\"Saving splits to Parquet files...\")\n",
    "        save_to_parquet(train_sampled_df, train_file)\n",
    "        save_to_parquet(val_df, val_file)\n",
    "        save_to_parquet(test_df, test_file)\n",
    "\n",
    "        logger.info(f\"Appended train data to: {train_file}\")\n",
    "        logger.info(f\"Appended val data to: {val_file}\")\n",
    "        logger.info(f\"Appended test data to: {test_file}\")\n",
    "        \n",
    "    else:\n",
    "        logger.warning(f\"Tweet file {tweet_file_path} does not exist.\")\n",
    "\n",
    "# Step 3: Validate saved files\n",
    "logger.info(\"Validating saved files...\")\n",
    "for file_path, split_name in zip([train_file, val_file, test_file], [\"Train\", \"Validation\", \"Test\"]):\n",
    "    split_df = spark.read.parquet(file_path)\n",
    "    logger.info(f\"{split_name} data: {split_df.count()} rows.\")\n",
    "    split_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "351a7150-f45b-4cde-9ab9-35e5859184cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.7 | packaged by conda-forge | (main, Oct  4 2024, 16:05:46) [GCC 13.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.7 (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
