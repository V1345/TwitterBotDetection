{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Bq1ZHer19rr"
      },
      "source": [
        "Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eURmrLs7BZz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "500e8b7c-c2eb-4b7e-a9d3-ab34a774426d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ijson\n",
            "  Downloading ijson-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
            "Collecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Collecting trl\n",
            "  Downloading trl-0.12.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.18)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.143)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.26.2)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (3.2.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.20.3)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.1.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (11.0.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (2024.9.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.18.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading ijson-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.12.1-py3-none-any.whl (310 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.9/310.9 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ijson, xxhash, fsspec, dill, multiprocess, bitsandbytes, datasets, trl, langchain_huggingface\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.44.1 datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 ijson-3.3.0 langchain_huggingface-0.1.2 multiprocess-0.70.16 trl-0.12.1 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install ijson bitsandbytes langchain langchain_huggingface datasets peft trl\n",
        "\n",
        "# General Python and ML libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import ast\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# NLP libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from transformers import (pipeline, AutoTokenizer, AutoModelForCausalLM, BertTokenizer,\n",
        "                          TFBertForSequenceClassification, BitsAndBytesConfig, TFBertModel,\n",
        "                          LlamaForCausalLM, LlamaTokenizer, TrainingArguments, logging)\n",
        "\n",
        "# Libraries for specific frameworks (LangChain, PEFT, Hugging Face, etc.)\n",
        "import bitsandbytes as bnb\n",
        "from langchain import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, PeftConfig, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer, setup_chat_format\n",
        "\n",
        "# Machine Learning libraries\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQYkDI-o18oz"
      },
      "source": [
        "Load a subset of tweets from the dataset and export to Google Drive for future use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFTTEm1sEW5n"
      },
      "outputs": [],
      "source": [
        "# # one time to generate a small subset of 10000 lines of tweets\n",
        "# def process_large_json(file_path, num_lines=100000):\n",
        "#   \"\"\"Reads a large JSON file line by line and processes the data.\n",
        "\n",
        "#   Args:\n",
        "#     file_path: Path to the JSON file.\n",
        "#     num_lines: The number of lines to process.\n",
        "#   \"\"\"\n",
        "#   tweets = []\n",
        "#   with open(file_path, 'r') as f:\n",
        "#     objects = ijson.items(f, 'item')\n",
        "#     count = 0\n",
        "#     for tweet in objects:\n",
        "#       if count < num_lines:\n",
        "#           tweets.append(tweet)\n",
        "#           count += 1\n",
        "#       else:\n",
        "#         break\n",
        "\n",
        "#   return tweets\n",
        "\n",
        "# file_path = '/content/drive/MyDrive/Twibot_22/tweet_8.json'\n",
        "# tweets = process_large_json(file_path)\n",
        "\n",
        "# tweets = pd.DataFrame(tweets)\n",
        "\n",
        "# labels = pd.read_csv('/content/drive/MyDrive/Twibot_22/label.csv')\n",
        "# split = pd.read_csv('/content/drive/MyDrive/Twibot_22/split.csv')\n",
        "\n",
        "# split = split.rename(columns={'id': 'split_id'})\n",
        "\n",
        "# tweets['user_id'] = 'u' + tweets['author_id'].astype(str)\n",
        "# tweets_en = tweets[tweets['lang'] == 'en']\n",
        "# tweets_labels = pd.merge(tweets_en, labels, left_on='user_id', right_on='id', how='inner')\n",
        "# tweets_labels = pd.merge(tweets_labels, split, left_on='user_id', right_on='split_id', how='inner')\n",
        "# # tweets_labels = tweets_labels[tweets_labels['split'].isin(['train', 'val'])]\n",
        "\n",
        "# tweets_labels = tweets_labels.rename(columns={'id_x': 'id'})\n",
        "# tweets_labels = tweets_labels.drop(['id_y', 'split_id'], axis=1)\n",
        "# print(tweets_labels.shape)\n",
        "\n",
        "# tweets_labels.to_csv('/content/drive/MyDrive/Twibot_22/tweet_8_subset_100k.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPLc6Ox-GRfw"
      },
      "outputs": [],
      "source": [
        "tweets_labels = pd.read_csv('/content/tweet_8_subset_100k.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1nvum7i_Xk1"
      },
      "outputs": [],
      "source": [
        "tweets_labels.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecN34E6WA0W7"
      },
      "outputs": [],
      "source": [
        "tweets_labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLfz9Z__KxIB"
      },
      "outputs": [],
      "source": [
        "tweets_labels.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_count = tweets_labels[tweets_labels['split'] == 'train'].shape[0]\n",
        "val_count = tweets_labels[tweets_labels['split'] == 'val'].shape[0]\n",
        "test_count = tweets_labels[tweets_labels['split'] == 'test'].shape[0]\n",
        "\n",
        "print(f\"Training count: {train_count}\")\n",
        "print(f\"Validation count: {val_count}\")\n",
        "print(f\"Test count: {test_count}\")"
      ],
      "metadata": {
        "id": "Ksn4AkAx-THg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = pd.read_csv('/content/label.csv')\n",
        "\n",
        "# Plotting Label Distribution (Human vs Bot)\n",
        "plt.figure(figsize=(6, 4))\n",
        "labels['label'].value_counts().plot(kind='bar')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Labels (Human vs Bot)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vMYWVVfi2OkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_labels['tweet_length'] = tweets_labels['text'].str.len()\n",
        "\n",
        "# Plotting Tweet Length Distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(tweets_labels['tweet_length'], bins=20)\n",
        "plt.xlabel('Tweet Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Tweet Lengths')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XscWWbOO2j2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgPbgEAGHRKW"
      },
      "outputs": [],
      "source": [
        "# full_text_bot_tweets = tweets_label['text'].tolist()\n",
        "# for text in full_text_bot_tweets:\n",
        "#   print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqVgU6oT30p4"
      },
      "source": [
        "Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_t839drL3zqI"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def clean_text(text):\n",
        "  \"\"\"Cleans text by removing URLs, mentions, hashtags, special characters,\n",
        "  converting to lowercase, removing 'rt', and applying stemming.\"\"\"\n",
        "  text = re.sub(r'\\bRT\\b' , \"\", text)\n",
        "  text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "  text = re.sub(r'@\\S+', '', text)  # Remove mentions\n",
        "  text = re.sub(r'#\\S+', '', text)  # Remove hashtags\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
        "  return text.strip()\n",
        "\n",
        "def preprocess_tweet(tweet):\n",
        "  \"\"\"Preprocesses a tweet by removing stop words, punctuation,\n",
        "  and applying stemming.\"\"\"\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=False)\n",
        "  tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "  stemmer = PorterStemmer()\n",
        "  filtered_tokens = [stemmer.stem(w) for w in tokens if w not in stop_words and w.isalnum()]\n",
        "  return \" \".join(filtered_tokens)\n",
        "\n",
        "def clean_and_preprocess(text):\n",
        "  \"\"\"Cleans and preprocesses text by removing URLs, mentions, hashtags,\n",
        "  special characters, converting to lowercase, removing 'rt', and applying\n",
        "  stemming.\"\"\"\n",
        "  text = clean_text(text)\n",
        "  text = preprocess_tweet(text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmmUYulVFWqU"
      },
      "source": [
        "Simple Tweets Models - Using only text data for predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLh7Xg6zKEzE"
      },
      "outputs": [],
      "source": [
        "df_reduced = tweets_labels[['user_id', 'text', 'label','split']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaGqEw3bKw7L"
      },
      "outputs": [],
      "source": [
        "df_reduced.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBL2wM_ML4Yt"
      },
      "outputs": [],
      "source": [
        "df_reduced['processed_text'] = df_reduced['text'].apply(clean_and_preprocess)\n",
        "\n",
        "df_reduced = df_reduced.drop(['text'], axis=1)\n",
        "\n",
        "df_reduced.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMgctOGFUpAj"
      },
      "outputs": [],
      "source": [
        "df_reduced.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMNSj6ESVBuy"
      },
      "outputs": [],
      "source": [
        "label_counts = df_reduced['label'].value_counts()\n",
        "print(label_counts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "X_train = df_reduced[tweets_labels['split'] == 'train']\n",
        "X_eval = df_reduced[tweets_labels['split'] == 'val']\n",
        "X_test = df_reduced[tweets_labels['split'] == 'test']"
      ],
      "metadata": {
        "id": "xPy5o7MGN9QS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.head()"
      ],
      "metadata": {
        "id": "kndw2diuRNUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q11bzhk4Pok"
      },
      "source": [
        "To address class imbalance, we employ sampling techniques to ensure balanced representation for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "no7dqmu8VKpE"
      },
      "outputs": [],
      "source": [
        "# # Separate human and bot tweets\n",
        "# human_tweets = df_reduced[df_reduced['label'] == 'human']\n",
        "# bot_tweets = df_reduced[df_reduced['label'] == 'bot']\n",
        "\n",
        "# # Determine the smaller number of human or bot tweets\n",
        "# min_count = min(len(human_tweets), len(bot_tweets))\n",
        "\n",
        "# # Sample the dataframes to have equal number of human and bot tweets\n",
        "# sampled_human_tweets = human_tweets.sample(n=min_count, random_state=42)\n",
        "# sampled_bot_tweets = bot_tweets.sample(n=min_count, random_state=42)\n",
        "\n",
        "# # Combine the sampled human and bot tweets\n",
        "# balanced_df = pd.concat([sampled_human_tweets, sampled_bot_tweets])\n",
        "\n",
        "# # Shuffle the dataframe to randomize the order\n",
        "# balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# print(balanced_df['label'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmOwuflw4f34"
      },
      "source": [
        "BERT Model: Utilized as a Baseline for Performance Evaluation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT tokenizer and model\n",
        "bert_basic_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_basic_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2) # 2 for binary classification (bot/human)\n",
        "\n",
        "# Prepare data for BERT\n",
        "def prepare_data(df):\n",
        "    texts = df['processed_text'].tolist()\n",
        "    labels = [1 if label == 'bot' else 0 for label in df['label'].tolist()]  # Convert labels to 0 and 1\n",
        "\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for text in texts:\n",
        "        encoded_dict = bert_basic_tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=64,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='tf',\n",
        "        )\n",
        "\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = tf.convert_to_tensor(input_ids)\n",
        "    attention_masks = tf.convert_to_tensor(attention_masks)\n",
        "    labels = tf.convert_to_tensor(labels)\n",
        "\n",
        "    # Remove extra dimension from tensors\n",
        "    input_ids = tf.squeeze(input_ids, axis=1)\n",
        "    attention_masks = tf.squeeze(attention_masks, axis=1)\n",
        "\n",
        "    return input_ids, attention_masks, labels\n",
        "\n",
        "# Prepare data for each split\n",
        "train_input_ids, train_attention_masks, train_labels = prepare_data(X_train)\n",
        "val_input_ids, val_attention_masks, val_labels = prepare_data(X_eval)\n",
        "test_input_ids, test_attention_masks, test_labels = prepare_data(X_test)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "# Compile the model\n",
        "bert_basic_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "\n",
        "# Train the model with validation data\n",
        "bert_basic_model.fit(\n",
        "    {'input_ids': train_input_ids, 'attention_mask': train_attention_masks},\n",
        "    train_labels,\n",
        "    validation_data=({'input_ids': val_input_ids, 'attention_mask': val_attention_masks}, val_labels),\n",
        "    batch_size=32,\n",
        "    epochs=3\n",
        ")\n",
        "\n",
        "bert_train_preds = bert_basic_model.predict({'input_ids': train_input_ids, 'attention_mask': train_attention_masks})\n",
        "bert_val_preds = bert_basic_model.predict({'input_ids': val_input_ids, 'attention_mask': val_attention_masks})\n",
        "bert_test_preds = bert_basic_model.predict({'input_ids': test_input_ids, 'attention_mask': test_attention_masks})\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "bert_basic_loss, bert_basic_accuracy = bert_basic_model.evaluate(\n",
        "    {'input_ids': test_input_ids, 'attention_mask': test_attention_masks},\n",
        "    test_labels\n",
        ")\n",
        "print(f'Loss: {bert_basic_loss}')\n",
        "print(f'Accuracy: {bert_basic_accuracy}')\n",
        "\n",
        "# Access logits and apply softmax to get probabilities\n",
        "logits = bert_test_preds.logits  # raw scores for each class\n",
        "probabilities = tf.nn.softmax(logits, axis=-1).numpy()  # convert to probabilities\n",
        "\n",
        "# Convert probabilities to binary predictions (0 or 1)\n",
        "y_pred_bert_basic = (probabilities[:, 1] > 0.5).astype(int)  # 1 if probability > 0.5, else 0"
      ],
      "metadata": {
        "id": "Jf2FzrQoViYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# ... [Your existing code up to predictions]\n",
        "\n",
        "# Convert probabilities to binary predictions (0 or 1)\n",
        "y_pred_bert_basic = (probabilities[:, 1] > 0.5).astype(int)  # 1 if probability > 0.5, else 0\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(test_labels.numpy(), y_pred_bert_basic)\n",
        "print(f'F1 Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "id": "28v4kWzyV0lZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear all variables except for loaded libraries and y_pred_bert_basic\n",
        "import gc\n",
        "\n",
        "# Get a list of all variables in the global scope\n",
        "variables = list(globals().keys())\n",
        "\n",
        "# Remove all variables except for y_pred_bert_basic and the loaded libraries\n",
        "for var in variables:\n",
        "    if var != 'y_pred_bert_basic' and not var.startswith('__'):\n",
        "        del globals()[var]\n",
        "\n",
        "# Clear garbage collected objects\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "wPsiwLdqc1eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNcsaroC4jg_"
      },
      "source": [
        "LLAMA 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZdFFV6w6uje"
      },
      "outputs": [],
      "source": [
        "# Load the LLaMA model and tokenizer\n",
        "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "# access_token = \"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\"\n",
        "\n",
        "access_token_rachit = \"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\"\n",
        "\n",
        "# quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda\", quantization_config=bnb_config, token = access_token_rachit)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, token=access_token_rachit)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Llama 3.1 training and fine tuning https://www.datacamp.com/tutorial/fine-tuning-llama-3-1"
      ],
      "metadata": {
        "id": "_XU13MHXC4Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"float16\",\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "9oJXqddBBPPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(test, model, tokenizer):\n",
        "    y_pred = []\n",
        "    categories = [\"human\", \"bot\"]\n",
        "\n",
        "    for i in tqdm(range(len(test))):\n",
        "        prompt = test.iloc[i][\"text\"]\n",
        "        pipe = pipeline(task=\"text-generation\",\n",
        "                        model=model,\n",
        "                        tokenizer=tokenizer,\n",
        "                        max_new_tokens=2,\n",
        "                        temperature=0.1)\n",
        "\n",
        "        result = pipe(prompt)\n",
        "        answer = result[0]['generated_text'].split(\"label:\")[-1].strip()\n",
        "\n",
        "        # Determine the predicted category\n",
        "        for category in categories:\n",
        "            if category.lower() in answer.lower():\n",
        "                y_pred.append(category)\n",
        "                break\n",
        "        else:\n",
        "            y_pred.append(\"none\")\n",
        "\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "yeiO0xGEBf-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(y_true, y_pred):\n",
        "    labels = [\"human\", \"bot\"]\n",
        "    mapping = {label: idx for idx, label in enumerate(labels)}\n",
        "\n",
        "    def map_func(x):\n",
        "        return mapping.get(x, -1)  # Map to -1 if not found, but should not occur with correct data\n",
        "\n",
        "    y_true_mapped = np.vectorize(map_func)(y_true)\n",
        "    y_pred_mapped = np.vectorize(map_func)(y_pred)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_true=y_true_mapped, y_pred=y_pred_mapped)\n",
        "    print(f'Accuracy: {accuracy:.3f}')\n",
        "\n",
        "    # Generate accuracy report\n",
        "    unique_labels = set(y_true_mapped)  # Get unique labels\n",
        "\n",
        "    for label in unique_labels:\n",
        "        label_indices = [i for i in range(len(y_true_mapped)) if y_true_mapped[i] == label]\n",
        "        label_y_true = [y_true_mapped[i] for i in label_indices]\n",
        "        label_y_pred = [y_pred_mapped[i] for i in label_indices]\n",
        "        label_accuracy = accuracy_score(label_y_true, label_y_pred)\n",
        "        print(f'Accuracy for label {labels[label]}: {label_accuracy:.3f}')\n",
        "\n",
        "    # Generate classification report\n",
        "    class_report = classification_report(y_true=y_true_mapped, y_pred=y_pred_mapped, target_names=labels, labels=list(range(len(labels))))\n",
        "    print('\\nClassification Report:')\n",
        "    print(class_report)\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_true=y_true_mapped, y_pred=y_pred_mapped, labels=list(range(len(labels))))\n",
        "    print('\\nConfusion Matrix:')\n",
        "    print(conf_matrix)"
      ],
      "metadata": {
        "id": "maIadW4hBsWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt generation functions\n",
        "def generate_prompt(data_point):\n",
        "    return f\"\"\"\n",
        "            Classify each twitter account as Bot or Human based on their tweet.\n",
        "text: {data_point[\"processed_text\"]}\n",
        "label: {data_point[\"label\"]}\"\"\".strip()\n",
        "\n",
        "def generate_test_prompt(data_point):\n",
        "    return f\"\"\"\n",
        "            Classify each twitter account as Bot or Human based on their tweet.\n",
        "text: {data_point[\"processed_text\"]}\n",
        "label: \"\"\".strip()\n",
        "\n",
        "# Generate prompts for training and evaluation data\n",
        "X_train.loc[:,'text'] = X_train.apply(generate_prompt, axis=1)\n",
        "X_eval.loc[:,'text'] = X_eval.apply(generate_prompt, axis=1)\n",
        "\n",
        "# Generate test prompts and extract true labels\n",
        "y_true = X_test.loc[:,'label']\n",
        "X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])"
      ],
      "metadata": {
        "id": "or63j1M79NiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.label.value_counts()"
      ],
      "metadata": {
        "id": "DosTCp0hAj2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to datasets\n",
        "train_data = Dataset.from_pandas(X_train[[\"text\"]])\n",
        "eval_data = Dataset.from_pandas(X_eval[[\"text\"]])"
      ],
      "metadata": {
        "id": "4DHfIGTpApRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_llama = predict(X_test, model, tokenizer)"
      ],
      "metadata": {
        "id": "9ihH3gTlQRMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(y_true, y_pred_llama)"
      ],
      "metadata": {
        "id": "az9jXrkfQSpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Llama Training"
      ],
      "metadata": {
        "id": "R6IZKIXNNPep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import bitsandbytes as bnb\n",
        "\n",
        "# def find_all_linear_names(model):\n",
        "#     cls = bnb.nn.Linear4bit\n",
        "#     lora_module_names = set()\n",
        "#     for name, module in model.named_modules():\n",
        "#         if isinstance(module, cls):\n",
        "#             names = name.split('.')\n",
        "#             lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "#     if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
        "#         lora_module_names.remove('lm_head')\n",
        "#     return list(lora_module_names)\n",
        "# modules = find_all_linear_names(model)\n",
        "# modules"
      ],
      "metadata": {
        "id": "uUGEof2DCbCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output_dir=\"/content/drive/MyDrive/Twibot_22/llama-3.1-fine-tuned-model\"\n",
        "\n",
        "# peft_config = LoraConfig(\n",
        "#     lora_alpha=16,\n",
        "#     lora_dropout=0,\n",
        "#     r=64,\n",
        "#     bias=\"none\",\n",
        "#     task_type=\"CAUSAL_LM\",\n",
        "#     target_modules=modules,\n",
        "# )\n",
        "\n",
        "# model.gradient_checkpointing_enable()\n",
        "# model= prepare_model_for_kbit_training(model)\n",
        "# peft_config.init_lora_weights = False\n",
        "# model.add_adapter(peft_config)\n",
        "\n",
        "# training_arguments = TrainingArguments(\n",
        "#     output_dir=output_dir,                    # directory to save and repository id\n",
        "#     num_train_epochs=1,                       # number of training epochs\n",
        "#     per_device_train_batch_size=1,            # batch size per device during training\n",
        "#     gradient_accumulation_steps=8,            # number of steps before performing a backward/update pass\n",
        "#     gradient_checkpointing=True,              # use gradient checkpointing to save memory\n",
        "#     optim=\"paged_adamw_32bit\",\n",
        "#     logging_steps=1,\n",
        "#     learning_rate=2e-4,                       # learning rate, based on QLoRA paper\n",
        "#     weight_decay=0.001,\n",
        "#     fp16=True,\n",
        "#     bf16=False,\n",
        "#     max_grad_norm=0.3,                        # max gradient norm based on QLoRA paper\n",
        "#     max_steps=-1,\n",
        "#     warmup_ratio=0.03,                        # warmup ratio based on QLoRA paper\n",
        "#     group_by_length=False,\n",
        "#     lr_scheduler_type=\"cosine\",               # use cosine learning rate scheduler\n",
        "#     report_to=\"wandb\",                  # report metrics to w&b\n",
        "#     eval_strategy=\"steps\",              # save checkpoint every epoch\n",
        "#     eval_steps = 0.2\n",
        "# )\n",
        "\n",
        "# trainer = SFTTrainer(\n",
        "#     model=model,\n",
        "#     args=training_arguments,\n",
        "#     train_dataset=train_data,\n",
        "#     eval_dataset=eval_data,\n",
        "#     peft_config=peft_config,\n",
        "#     dataset_text_field=\"text\",\n",
        "#     tokenizer=tokenizer,\n",
        "#     max_seq_length=512,\n",
        "#     packing=False,\n",
        "#     dataset_kwargs={\n",
        "#     \"add_special_tokens\": False,\n",
        "#     \"append_concat_token\": False,\n",
        "#     }\n",
        "# )"
      ],
      "metadata": {
        "id": "rkOac_4vCicp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.train()"
      ],
      "metadata": {
        "id": "u2xKrQP0Cq_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb.finish()\n",
        "# model.config.use_cache = True"
      ],
      "metadata": {
        "id": "G-RSwlW-Cu_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save trained model and tokenizer\n",
        "# trainer.save_model(output_dir)\n",
        "# tokenizer.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "kd1dIEtJCw_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_pred = predict(X_test, model, tokenizer)\n",
        "# evaluate(y_true, y_pred)"
      ],
      "metadata": {
        "id": "b0HQQXeJCy7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Llama 3.1 in-context"
      ],
      "metadata": {
        "id": "slsnyKhKPden"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt generation functions with k in-context samples\n",
        "def generate_prompt_incontext(data_point, X_train, k=5):\n",
        "    # Sample k random examples from X_train\n",
        "    examples = X_train.sample(n=k)\n",
        "    example_prompts = \"\\n\".join([\n",
        "        f\"tweet: {ex['processed_text']}\\nlabel: {ex['label']}\"\n",
        "        for _, ex in examples.iterrows()\n",
        "    ])\n",
        "\n",
        "    # Append the current data point at the end of the prompt\n",
        "    return f\"\"\"\n",
        "            Classify each twitter account as Bot or Human based on their tweet, above we have attached few samples.\n",
        "{example_prompts}\n",
        "text: {data_point['processed_text']}\n",
        "label: {data_point['label']}\n",
        "    \"\"\".strip()\n",
        "\n",
        "def generate_test_prompt_incontext(data_point, X_train, k=5):\n",
        "    # Sample k random examples from X_train\n",
        "    examples = X_train.sample(n=k)\n",
        "    example_prompts = \"\\n\".join([\n",
        "        f\"text: {ex['processed_text']}\\nlabel: {ex['label']}\"\n",
        "        for _, ex in examples.iterrows()\n",
        "    ])\n",
        "\n",
        "    # Append the current test data point at the end of the prompt, with an empty label for prediction\n",
        "    return f\"\"\"\n",
        "            Classify each twitter account as Bot or Human based on their tweet, above we have attached few samples.\n",
        "{example_prompts}\n",
        "text: {data_point['processed_text']}\n",
        "label:\n",
        "    \"\"\".strip()\n",
        "\n",
        "# Generate prompts for training and evaluation data\n",
        "# X_train['text'] = X_train.apply(lambda dp: generate_prompt_incontext(dp, X_train, k=5), axis=1)\n",
        "# X_eval['text'] = X_eval.apply(lambda dp: generate_prompt_incontext(dp, X_train, k=5), axis=1)\n",
        "\n",
        "# Generate test prompts and extract true labels\n",
        "y_true = X_test['label']\n",
        "X_test = pd.DataFrame(X_test.apply(lambda dp: generate_test_prompt_incontext(dp, X_train, k=5), axis=1), columns=[\"text\"])"
      ],
      "metadata": {
        "id": "B_33J6tGOpmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to datasets\n",
        "# train_data = Dataset.from_pandas(X_train[[\"incontext_text\"]])\n",
        "# eval_data = Dataset.from_pandas(X_eval[[\"incontext_text\"]])"
      ],
      "metadata": {
        "id": "fohWooZ-PzAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_llama_incontext = predict(X_test, model, tokenizer)"
      ],
      "metadata": {
        "id": "Ot18RPQOP1yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(y_true, y_pred_llama_incontext)"
      ],
      "metadata": {
        "id": "U9tVSRJzQAcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(y_true, y_pred_bert_basic)"
      ],
      "metadata": {
        "id": "vChnQj-SZs0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Old - Do not use or delete"
      ],
      "metadata": {
        "id": "SLhtkkBjOY2X"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbAtt4IT-1YB"
      },
      "source": [
        "## Enhanced Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrnrnvLE4muv"
      },
      "source": [
        "Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vk-wPQtnFbVB"
      },
      "outputs": [],
      "source": [
        "tweets_labels.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0iC0235fBlN"
      },
      "outputs": [],
      "source": [
        "tweets_labels.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXnqVP2O46v6"
      },
      "source": [
        "Extracting public metrics to compute summary statistics at the user level. Extending tweet-level data with user-level insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCk4Ldkyfs_R"
      },
      "outputs": [],
      "source": [
        "tweets_labels['public_metrics'] = tweets_labels['public_metrics'].astype(str).str.replace('None', \"'None'\", regex=False)\n",
        "tweets_labels['public_metrics'] = tweets_labels['public_metrics'].apply(ast.literal_eval)\n",
        "\n",
        "# Extract each value into separate columns\n",
        "tweets_labels['retweet_count'] = tweets_labels['public_metrics'].apply(lambda x: x.get('retweet_count', 0) if x is not None else 0)\n",
        "tweets_labels['reply_count'] = tweets_labels['public_metrics'].apply(lambda x: x.get('reply_count', 0) if x is not None else 0)\n",
        "tweets_labels['like_count'] = tweets_labels['public_metrics'].apply(lambda x: x.get('like_count', 0) if x is not None else 0)\n",
        "tweets_labels['quote_count'] = tweets_labels['public_metrics'].apply(lambda x: x.get('quote_count', 0) if x is not None else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEo6XjxNHGxk"
      },
      "outputs": [],
      "source": [
        "tweets_labels[['retweet_count', 'reply_count', 'like_count', 'quote_count']] = \\\n",
        "    tweets_labels[['retweet_count', 'reply_count', 'like_count', 'quote_count']].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "tweets_labels[['retweet_count', 'reply_count', 'like_count', 'quote_count']] = tweets_labels[['retweet_count', 'reply_count', 'like_count', 'quote_count']].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U10DYFdbEtty"
      },
      "outputs": [],
      "source": [
        "tweets_labels.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaPmTh2oZvi3"
      },
      "outputs": [],
      "source": [
        "# Convert 'created_at' to datetime objects\n",
        "tweets_labels['created_at'] = pd.to_datetime(tweets_labels['created_at'])\n",
        "\n",
        "# Access retweet count using the apply method\n",
        "def get_retweet_count(x):\n",
        "    try:\n",
        "        return x['public_metrics']['retweet_count']\n",
        "    except (KeyError, TypeError):\n",
        "        return 0\n",
        "\n",
        "# Group by author and calculate the required metrics\n",
        "author_summary = tweets_labels.groupby('user_id').agg(\n",
        "    total_tweets=('text', 'count'),\n",
        "    last_day_tweets=('created_at', lambda x: (x >= (pd.Timestamp.now(tz='UTC') - pd.DateOffset(days=1))).sum()),\n",
        "    last_week_tweets=('created_at', lambda x: (x >= (pd.Timestamp.now(tz='UTC') - pd.DateOffset(weeks=1))).sum()),\n",
        "    last_month_tweets=('created_at', lambda x: (x >= (pd.Timestamp.now(tz='UTC') - pd.DateOffset(months=1))).sum()),\n",
        "    days_since_first_tweet=('created_at', lambda x: (pd.Timestamp.now(tz='UTC') - x.min()).days),\n",
        "    days_since_last_tweet=('created_at', lambda x: (pd.Timestamp.now(tz='UTC') - x.max()).days),\n",
        "    total_retweets=('retweet_count', 'sum'),\n",
        "    total_replies=('reply_count', 'sum'),\n",
        "    total_likes=('like_count', 'sum'),\n",
        "    total_quotes=('quote_count', 'sum')\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNNto4qAcfm-"
      },
      "outputs": [],
      "source": [
        "author_summary.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vehCpRrVgvOG"
      },
      "outputs": [],
      "source": [
        "# Merge author_summary with tweets_labels on 'user_id'\n",
        "tweets_labels_with_summary = pd.merge(tweets_labels, author_summary, on='user_id', how='left')\n",
        "\n",
        "# Print the merged dataframe\n",
        "tweets_labels_with_summary.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEAnL0bn5y3_"
      },
      "source": [
        "Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVlA4P99h6FS"
      },
      "outputs": [],
      "source": [
        "tweets_labels_with_summary['processed_text'] = tweets_labels_with_summary['text'].apply(clean_and_preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcETjEa0g6lT"
      },
      "outputs": [],
      "source": [
        "tweets_labels_with_summary = tweets_labels_with_summary.drop(['lang', 'attachments', 'author_id', 'conversation_id', 'geo', 'id', 'in_reply_to_user_id', 'reply_settings', 'source', 'referenced_tweets', 'withheld', 'public_metrics', 'text'], axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMCBdOgn6Aqi"
      },
      "source": [
        "Using the BERT model (though not the ideal strategy) as it allows us to compare baselines. We appended additional feature tensors to BERT's text embeddings for enhanced representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wMxWOBJpAwa"
      },
      "source": [
        "BERT's accuracy using only text data and text with additional features is fluctuating a lot. Need to check for overfitting."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare LightGBM features\n",
        "def prepare_lgbm_data(df):\n",
        "    features = df[['retweet_count', 'reply_count', 'like_count', 'quote_count', 'total_tweets',\n",
        "                   'last_day_tweets', 'last_week_tweets', 'last_month_tweets', 'days_since_first_tweet',\n",
        "                   'days_since_last_tweet', 'total_retweets', 'total_replies', 'total_likes']].values\n",
        "    labels = [1 if label == 'bot' else 0 for label in df['label'].tolist()]\n",
        "    return features, labels\n",
        "\n",
        "# Preparing data\n",
        "train_input_ids, train_attention_ids = prepare_bert_data(X_train['processed_text'].tolist())\n",
        "\n",
        "train_lgbm_features, train_lgbm_labels = prepare_lgbm_data(X_train)\n",
        "val_lgbm_features, val_lgbm_labels = prepare_lgbm_data(X_eval)\n",
        "test_lgbm_features, test_lgbm_labels = prepare_lgbm_data(X_test)\n",
        "\n",
        "# Train LightGBM model\n",
        "train_data = lgb.Dataset(train_lgbm_features, label=train_lgbm_labels)\n",
        "val_data = lgb.Dataset(val_lgbm_features, label=val_lgbm_labels)\n",
        "test_data = lgb.Dataset(test_lgbm_features, label=test_lgbm_labels)\n",
        "\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9\n",
        "}\n",
        "\n",
        "lgb_model = lgb.train(params, train_data, num_boost_round=100, valid_sets=[val_data], early_stopping_rounds=10)\n",
        "\n",
        "# Get LightGBM predictions\n",
        "lgb_train_preds = lgb_model.predict(train_lgbm_features)\n",
        "lgb_val_preds = lgb_model.predict(val_lgbm_features)\n",
        "lgb_test_preds = lgb_model.predict(test_lgbm_features)\n",
        "\n",
        "# Ensemble predictions by averaging BERT and LightGBM outputs\n",
        "ensemble_train_preds = (bert_train_preds.logits[:, 1] + lgb_train_preds) / 2\n",
        "ensemble_val_preds = (bert_val_preds.logits[:, 1] + lgb_val_preds) / 2\n",
        "ensemble_test_preds = (bert_test_preds.logits[:, 1] + lgb_test_preds) / 2\n",
        "\n",
        "# Convert predictions to binary labels (0 or 1)\n",
        "ensemble_train_labels = (ensemble_train_preds > 0.5).astype(int)\n",
        "ensemble_val_labels = (ensemble_val_preds > 0.5).astype(int)\n",
        "ensemble_test_labels = (ensemble_test_preds > 0.5).astype(int)\n",
        "\n",
        "# Evaluate ensemble model\n",
        "train_accuracy = accuracy_score(train_lgbm_labels, ensemble_train_labels)\n",
        "val_accuracy = accuracy_score(val_lgbm_labels, ensemble_val_labels)\n",
        "test_accuracy = accuracy_score(test_lgbm_labels, ensemble_test_labels)\n",
        "\n",
        "print(f\"Training Accuracy: {train_accuracy}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")\n",
        "print(f\"Testing Accuracy: {test_accuracy}\")\n",
        "\n",
        "bert_ensemble_lgbm_accuracy = test_accuracy"
      ],
      "metadata": {
        "id": "pCWyix9SXfu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(y_true, ensemble_test_labels)"
      ],
      "metadata": {
        "id": "V0N00xAsZ12n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HecqbDPIcCZG"
      },
      "outputs": [],
      "source": [
        "basic_bert_accuracy = (y_pred_bert_basic == y_true).mean()\n",
        "llama_3_accuracy = (y_pred_llama == y_true).mean()\n",
        "llama_3_incontext_accuracy = (y_pred_llama_incontext == y_true).mean()\n",
        "bert_ensemble_lgbm_accuracy = (ensemble_test_labels == y_true).mean()\n",
        "\n",
        "accuracies = [basic_bert_accuracy, llama_3_accuracy, llama_3_incontext_accuracy, bert_ensemble_lgbm_accuracy]\n",
        "model_names = ['BERT', 'LLAMA 3 Accuracy', 'LLAMA 3 with Incontext Examples Accuracy', 'BERT Ensemble with LGBM']\n",
        "\n",
        "# Create a bar graph\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(model_names, accuracies)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Comparison of Model Accuracies')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WORKING BELOW IS DONE AFTER PROJECT STATUS REPORT SUBMISSION"
      ],
      "metadata": {
        "id": "pJ6WF7nL2Lsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_labels = pd.read_csv('/content/tweet_8_subset_100k.csv')"
      ],
      "metadata": {
        "id": "hqPJ0NLL2QAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_labels.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "5a0epEO62ca5",
        "outputId": "1cc54b79-e539-466d-8383-53dc73784291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   attachments           author_id  context_annotations      conversation_id  \\\n",
              "0          NaN  730877400662212609                  NaN  1391411268519616518   \n",
              "1          NaN  730877400662212609                  NaN  1390722859757096969   \n",
              "2          NaN  730877400662212609                  NaN  1390722788554645506   \n",
              "3          NaN  730877400662212609                  NaN  1390417994325577733   \n",
              "4          NaN  730877400662212609                  NaN  1390292998584520711   \n",
              "\n",
              "                  created_at  \\\n",
              "0  2021-05-09 15:14:32+00:00   \n",
              "1  2021-05-07 17:39:02+00:00   \n",
              "2  2021-05-07 17:38:45+00:00   \n",
              "3  2021-05-06 21:27:37+00:00   \n",
              "4  2021-05-06 13:10:55+00:00   \n",
              "\n",
              "                                            entities  geo  \\\n",
              "0  {'hashtags': [], 'symbols': [], 'user_mentions...  NaN   \n",
              "1  {'hashtags': [], 'symbols': [], 'user_mentions...  NaN   \n",
              "2  {'hashtags': [], 'symbols': [], 'user_mentions...  NaN   \n",
              "3  {'hashtags': [], 'symbols': [], 'user_mentions...  NaN   \n",
              "4  {'hashtags': [{'text': 'ALBUMPREMIERE', 'indic...  NaN   \n",
              "\n",
              "                     id  in_reply_to_user_id lang  possibly_sensitive  \\\n",
              "0  t1391411268519616518                  NaN   en               False   \n",
              "1  t1390722859757096969                  NaN   en               False   \n",
              "2  t1390722788554645506                  NaN   en               False   \n",
              "3  t1390417994325577733                  NaN   en               False   \n",
              "4  t1390292998584520711                  NaN   en               False   \n",
              "\n",
              "                                      public_metrics  referenced_tweets  \\\n",
              "0  {'retweet_count': 1, 'reply_count': None, 'lik...                NaN   \n",
              "1  {'retweet_count': 1, 'reply_count': None, 'lik...                NaN   \n",
              "2  {'retweet_count': 3, 'reply_count': None, 'lik...                NaN   \n",
              "3  {'retweet_count': 2, 'reply_count': None, 'lik...                NaN   \n",
              "4  {'retweet_count': 2, 'reply_count': None, 'lik...                NaN   \n",
              "\n",
              "   reply_settings                                             source  \\\n",
              "0             NaN  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
              "1             NaN  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
              "2             NaN  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
              "3             NaN  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
              "4             NaN  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
              "\n",
              "                                                text  withheld  \\\n",
              "0  Brand new R. Missing interview with Spain's @m...       NaN   \n",
              "1  RT @GavinHavery: Kicking off tonight's Radio B...       NaN   \n",
              "2  RT @puddlegum: R. Missing - @rmissingmusic - r...       NaN   \n",
              "3  RT @BigTakeoverMag: Album Premiere: Crimeless ...       NaN   \n",
              "4  RT @JenStratosphere: MY #ALBUMPREMIERE of atmo...       NaN   \n",
              "\n",
              "               user_id  label split  \n",
              "0  u730877400662212609  human   val  \n",
              "1  u730877400662212609  human   val  \n",
              "2  u730877400662212609  human   val  \n",
              "3  u730877400662212609  human   val  \n",
              "4  u730877400662212609  human   val  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8d4e4e86-3a97-4ac3-8b53-757d3b826ae0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>attachments</th>\n",
              "      <th>author_id</th>\n",
              "      <th>context_annotations</th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>entities</th>\n",
              "      <th>geo</th>\n",
              "      <th>id</th>\n",
              "      <th>in_reply_to_user_id</th>\n",
              "      <th>lang</th>\n",
              "      <th>possibly_sensitive</th>\n",
              "      <th>public_metrics</th>\n",
              "      <th>referenced_tweets</th>\n",
              "      <th>reply_settings</th>\n",
              "      <th>source</th>\n",
              "      <th>text</th>\n",
              "      <th>withheld</th>\n",
              "      <th>user_id</th>\n",
              "      <th>label</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>730877400662212609</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1391411268519616518</td>\n",
              "      <td>2021-05-09 15:14:32+00:00</td>\n",
              "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>t1391411268519616518</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>False</td>\n",
              "      <td>{'retweet_count': 1, 'reply_count': None, 'lik...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
              "      <td>Brand new R. Missing interview with Spain's @m...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>u730877400662212609</td>\n",
              "      <td>human</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>730877400662212609</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1390722859757096969</td>\n",
              "      <td>2021-05-07 17:39:02+00:00</td>\n",
              "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>t1390722859757096969</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>False</td>\n",
              "      <td>{'retweet_count': 1, 'reply_count': None, 'lik...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
              "      <td>RT @GavinHavery: Kicking off tonight's Radio B...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>u730877400662212609</td>\n",
              "      <td>human</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>730877400662212609</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1390722788554645506</td>\n",
              "      <td>2021-05-07 17:38:45+00:00</td>\n",
              "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>t1390722788554645506</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>False</td>\n",
              "      <td>{'retweet_count': 3, 'reply_count': None, 'lik...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
              "      <td>RT @puddlegum: R. Missing - @rmissingmusic - r...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>u730877400662212609</td>\n",
              "      <td>human</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>730877400662212609</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1390417994325577733</td>\n",
              "      <td>2021-05-06 21:27:37+00:00</td>\n",
              "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>t1390417994325577733</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>False</td>\n",
              "      <td>{'retweet_count': 2, 'reply_count': None, 'lik...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
              "      <td>RT @BigTakeoverMag: Album Premiere: Crimeless ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>u730877400662212609</td>\n",
              "      <td>human</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>730877400662212609</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1390292998584520711</td>\n",
              "      <td>2021-05-06 13:10:55+00:00</td>\n",
              "      <td>{'hashtags': [{'text': 'ALBUMPREMIERE', 'indic...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>t1390292998584520711</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>False</td>\n",
              "      <td>{'retweet_count': 2, 'reply_count': None, 'lik...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
              "      <td>RT @JenStratosphere: MY #ALBUMPREMIERE of atmo...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>u730877400662212609</td>\n",
              "      <td>human</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d4e4e86-3a97-4ac3-8b53-757d3b826ae0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8d4e4e86-3a97-4ac3-8b53-757d3b826ae0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8d4e4e86-3a97-4ac3-8b53-757d3b826ae0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-446372d3-e731-4d76-9f14-f09063d0dd92\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-446372d3-e731-4d76-9f14-f09063d0dd92')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-446372d3-e731-4d76-9f14-f09063d0dd92 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "tweets_labels",
              "summary": "{\n  \"name\": \"tweets_labels\",\n  \"rows\": 5125,\n  \"fields\": [\n    {\n      \"column\": \"attachments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"author_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 566114742175491136,\n        \"min\": 14049214,\n        \"max\": 1484509576028381186,\n        \"num_unique_values\": 254,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_annotations\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"conversation_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 218385686274535008,\n        \"min\": 72809384459247616,\n        \"max\": 1496976434417606662,\n        \"num_unique_values\": 5125,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"created_at\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 5088,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"entities\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4455,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"geo\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5125,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"in_reply_to_user_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.673622693429559e+17,\n        \"min\": 409823.0,\n        \"max\": 1.4967679696358892e+18,\n        \"num_unique_values\": 1064,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lang\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"possibly_sensitive\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"public_metrics\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1123,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"referenced_tweets\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reply_settings\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 40,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5088,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"withheld\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"user_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 254,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"split\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_count = tweets_labels[tweets_labels['split'] == 'train'].shape[0]\n",
        "val_count = tweets_labels[tweets_labels['split'] == 'val'].shape[0]\n",
        "test_count = tweets_labels[tweets_labels['split'] == 'test'].shape[0]\n",
        "\n",
        "print(f\"Training count: {train_count}\")\n",
        "print(f\"Validation count: {val_count}\")\n",
        "print(f\"Test count: {test_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4FG0wAQ40TP",
        "outputId": "fcab12cb-615b-4a95-c661-768d93c8c589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training count: 4208\n",
            "Validation count: 660\n",
            "Test count: 256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_rows = len(tweets_labels)\n",
        "num_splits = 3\n",
        "split_size = num_rows // num_splits\n",
        "\n",
        "# Split the dataset into three parts\n",
        "for i in range(num_splits):\n",
        "    start_idx = i * split_size\n",
        "    end_idx = start_idx + split_size if i < num_splits - 1 else num_rows  # Ensure the last split gets remaining rows\n",
        "    split = tweets_labels.iloc[start_idx:end_idx]\n",
        "    split_file_path = f'tweet_subset_split_{i+1}.csv'  # Output filenames\n",
        "    split.to_csv(split_file_path, index=False)\n",
        "    print(f\"Saved: {split_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuKXSjUz2vND",
        "outputId": "6b0e4170-4019-4b77-a589-2a74a857d9a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: tweet_subset_split_1.csv\n",
            "Saved: tweet_subset_split_2.csv\n",
            "Saved: tweet_subset_split_3.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_1 = pd.read_csv('/content/tweet_subset_split_1.csv')\n",
        "tweets_2 = pd.read_csv('/content/tweet_subset_split_2.csv')\n",
        "tweets_3 = pd.read_csv('/content/tweet_subset_split_3.csv')\n",
        "\n",
        "split = pd.read_csv('/content/split.csv')\n",
        "\n",
        "labels = pd.read_csv('/content/label.csv')"
      ],
      "metadata": {
        "id": "_kHnv7HK3WnR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "28432418-eac2-46c1-e8e2-1674c46cfabe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/tweet_subset_split_1.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-e6227f706a86>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweets_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/tweet_subset_split_1.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtweets_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/tweet_subset_split_2.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtweets_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/tweet_subset_split_3.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/split.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/tweet_subset_split_1.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = pd.read_csv('/content/label.csv')\n",
        "labels.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "uw2ch6GuMtqm",
        "outputId": "d1416af1-d573-4e9b-d635-03ade56ca30c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     id  label\n",
              "0  u1217628182611927040  human\n",
              "1           u2664730894  human\n",
              "2  u1266703520205549568  human\n",
              "3  u1089159225148882949  human\n",
              "4             u36741729    bot"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ce3f0e63-5cd5-427e-9f34-ee58b186ba02\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>u1217628182611927040</td>\n",
              "      <td>human</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>u2664730894</td>\n",
              "      <td>human</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>u1266703520205549568</td>\n",
              "      <td>human</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>u1089159225148882949</td>\n",
              "      <td>human</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>u36741729</td>\n",
              "      <td>bot</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce3f0e63-5cd5-427e-9f34-ee58b186ba02')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ce3f0e63-5cd5-427e-9f34-ee58b186ba02 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ce3f0e63-5cd5-427e-9f34-ee58b186ba02');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f757ccda-5845-4ae4-a844-3fc0e3803148\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f757ccda-5845-4ae4-a844-3fc0e3803148')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f757ccda-5845-4ae4-a844-3fc0e3803148 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "labels"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "cqx35ggC3gE5",
        "outputId": "50a92ffb-f13a-4fb4-95ed-93677dbaeb19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     id  split\n",
              "0           u2664730894  train\n",
              "1  u1089159225148882949  train\n",
              "2             u36741729  train\n",
              "3           u1679822588  train\n",
              "4           u1519144464  train"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2d6cef52-db9a-4389-87d2-56e319789061\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>u2664730894</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>u1089159225148882949</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>u36741729</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>u1679822588</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>u1519144464</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2d6cef52-db9a-4389-87d2-56e319789061')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2d6cef52-db9a-4389-87d2-56e319789061 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2d6cef52-db9a-4389-87d2-56e319789061');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ed350350-4a5d-4eeb-b9e0-1fcd6940db10\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ed350350-4a5d-4eeb-b9e0-1fcd6940db10')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ed350350-4a5d-4eeb-b9e0-1fcd6940db10 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "split"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "t5XjzF-35L8w",
        "outputId": "359e98ca-6b40-45f0-a187-1814d81b00b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ijson\n",
        "import json\n",
        "\n",
        "from decimal import Decimal\n",
        "\n",
        "def process_and_split_large_json(file_path, output_prefix, num_splits=3, split_size=30000):\n",
        "    \"\"\"Reads a large JSON file line by line, processes the data, and saves splits.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to the JSON file.\n",
        "        output_prefix: Prefix for the output files.\n",
        "        num_splits: Number of splits to create.\n",
        "        split_size: Number of rows in each split.\n",
        "    \"\"\"\n",
        "    def convert_to_serializable(obj):\n",
        "        \"\"\"Convert non-serializable objects to a serializable format.\"\"\"\n",
        "        if isinstance(obj, Decimal):\n",
        "            return float(obj)  # or str(obj) if you prefer\n",
        "        raise TypeError(f\"Object of type {obj.__class__.__name__} is not JSON serializable\")\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "        objects = ijson.items(f, 'item')\n",
        "\n",
        "        tweets = []\n",
        "        count = 0\n",
        "        file_count = 1\n",
        "\n",
        "        for tweet in objects:\n",
        "            if count < split_size:\n",
        "                tweets.append(tweet)\n",
        "                count += 1\n",
        "            else:\n",
        "                # Save the current split\n",
        "                output_file = f\"{output_prefix}_split_{file_count}.json\"\n",
        "                with open(output_file, 'w') as out_f:\n",
        "                    json.dump(tweets, out_f, indent=2, default=convert_to_serializable)\n",
        "                print(f\"Saved: {output_file}\")\n",
        "\n",
        "                # Prepare for the next split\n",
        "                tweets = [tweet]\n",
        "                count = 1\n",
        "                file_count += 1\n",
        "\n",
        "                # Stop if all splits are completed\n",
        "                if file_count > num_splits:\n",
        "                    break\n",
        "\n",
        "        # Save the remaining tweets (if any)\n",
        "        if tweets and file_count <= num_splits:\n",
        "            output_file = f\"{output_prefix}_split_{file_count}.json\"\n",
        "            with open(output_file, 'w') as out_f:\n",
        "                json.dump(tweets, out_f, indent=2, default=convert_to_serializable)\n",
        "            print(f\"Saved: {output_file}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "process_and_split_large_json(\n",
        "    file_path = '/content/drive/MyDrive/Twibot_22/tweet_0.json',  # Replace with your file path\n",
        "    output_prefix='tweet_subset',  # Prefix for the output files\n",
        "    num_splits=3,                  # Number of splits\n",
        "    split_size=30000               # Number of tweets per split\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gQxSK0H59jM",
        "outputId": "eaf12749-8903-41b6-dfa6-ea5f63d07b28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: tweet_subset_split_1.json\n",
            "Saved: tweet_subset_split_2.json\n",
            "Saved: tweet_subset_split_3.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CODE BELOW ITERATES OVER TWEET FILE, ALSO TAKES IN LABEL AND SPLIT FILES...AND THEN GENERATES PROCESSED TWEET FILES WITH 5 TWEETS PER USER."
      ],
      "metadata": {
        "id": "-Ueg0ARfO2_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_large_json(file_path, num_lines=100000):\n",
        "  \"\"\"Reads a large JSON file line by line and processes the data.\n",
        "\n",
        "  Args:\n",
        "    file_path: Path to the JSON file.\n",
        "    num_lines: The number of lines to process.\n",
        "  \"\"\"\n",
        "  tweets = []\n",
        "  with open(file_path, 'r') as f:\n",
        "    objects = ijson.items(f, 'item')\n",
        "    count = 0\n",
        "    for tweet in objects:\n",
        "      if count < num_lines:\n",
        "          tweets.append(tweet)\n",
        "          count += 1\n",
        "      else:\n",
        "        break\n",
        "\n",
        "  return tweets"
      ],
      "metadata": {
        "id": "nDo7OZAs9ocl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import random\n",
        "\n",
        "files = ['tweet_subset_split_1.json', 'tweet_subset_split_2.json', 'tweet_subset_split_3.json']\n",
        "\n",
        "labels = pd.read_csv('/content/drive/MyDrive/Twibot_22/label.csv')\n",
        "split = pd.read_csv('/content/drive/MyDrive/Twibot_22/split.csv')\n",
        "\n",
        "split = split.rename(columns={'id': 'split_id'})\n",
        "\n",
        "train_file = 'tweets_train.json'\n",
        "val_file = 'tweets_val.json'\n",
        "test_file = 'tweets_test.json'\n",
        "\n",
        "# Remove files if they already exist to start fresh\n",
        "for file_path in [train_file, val_file, test_file]:\n",
        "    if os.path.exists(f'/content/{file_path}'):\n",
        "        os.remove(f'/content/{file_path}')\n",
        "\n",
        "# Initialize empty lists to store all tweet data for each split\n",
        "train_data = []\n",
        "val_data = []\n",
        "test_data = []\n",
        "\n",
        "# Process each tweet file\n",
        "for file_name in files:\n",
        "    file_path = f'/content/{file_name}'\n",
        "    tweets = process_large_json(file_path)\n",
        "\n",
        "    tweets = pd.DataFrame(tweets)\n",
        "\n",
        "    print(f\"Processing {file_name}, shape: {tweets.shape}\")\n",
        "\n",
        "    tweets['user_id'] = 'u' + tweets['author_id'].astype(str)\n",
        "    tweets = tweets[tweets['lang'] == 'en']\n",
        "    tweets = tweets[['user_id', 'text']]\n",
        "\n",
        "    # Merge with labels and split data\n",
        "    tweets_labels = pd.merge(tweets, labels, left_on='user_id', right_on='id', how='inner')\n",
        "    tweets_labels = pd.merge(tweets_labels, split, left_on='user_id', right_on='split_id', how='inner')\n",
        "    tweets_labels = tweets_labels.drop(['id', 'split_id'], axis=1)\n",
        "\n",
        "    # Filter data by split (train, val, test)\n",
        "    tweets_labels_train = tweets_labels[tweets_labels['split'].isin(['train'])].drop_duplicates()\n",
        "    tweets_labels_val = tweets_labels[tweets_labels['split'].isin(['val'])].drop_duplicates()\n",
        "    tweets_labels_test = tweets_labels[tweets_labels['split'].isin(['test'])].drop_duplicates()\n",
        "\n",
        "    # Select 5 random tweets per user in the training data only\n",
        "    tweets_labels_train = tweets_labels_train.groupby('user_id').apply(lambda x: x.sample(n=min(5, len(x)), random_state=42)).reset_index(drop=True)\n",
        "\n",
        "    print('train', tweets_labels_train.shape)\n",
        "    print('val', tweets_labels_val.shape)\n",
        "    print('test', tweets_labels_test.shape)\n",
        "\n",
        "    # Append the data to the lists\n",
        "    train_data.append(tweets_labels_train.to_dict(orient='records'))\n",
        "    val_data.append(tweets_labels_val.to_dict(orient='records'))\n",
        "    test_data.append(tweets_labels_test.to_dict(orient='records'))\n",
        "\n",
        "    print(f\"Processed and added data for {file_name}\")\n",
        "    print()\n",
        "\n",
        "# Save each split's data as JSON\n",
        "with open(train_file, 'w') as f_train:\n",
        "    json.dump([item for sublist in train_data for item in sublist], f_train, indent=4)\n",
        "\n",
        "with open(val_file, 'w') as f_val:\n",
        "    json.dump([item for sublist in val_data for item in sublist], f_val, indent=4)\n",
        "\n",
        "with open(test_file, 'w') as f_test:\n",
        "    json.dump([item for sublist in test_data for item in sublist], f_test, indent=4)\n",
        "\n",
        "print(f\"Saved train data to {train_file}\")\n",
        "print(f\"Saved val data to {val_file}\")\n",
        "print(f\"Saved test data to {test_file}\")\n",
        "\n",
        "# Optionally, read back the files and check if everything is correct\n",
        "for file_path in [train_file, val_file, test_file]:\n",
        "    try:\n",
        "        with open(f'/content/{file_path}', 'r') as f:\n",
        "            data = json.load(f)\n",
        "            print(f\"Read {file_path}, total records:\", len(data))\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybSQ_s9Y4rlT",
        "outputId": "15b928d7-dd75-4efe-e2bc-3d972b1161d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing tweet_subset_split_1.json, shape: (30000, 17)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-79-bedc56846792>:50: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  tweets_labels_train = tweets_labels_train.groupby('user_id').apply(lambda x: x.sample(n=min(5, len(x)), random_state=42)).reset_index(drop=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train (2761, 4)\n",
            "val (3655, 4)\n",
            "test (4363, 4)\n",
            "Processed and added data for tweet_subset_split_1.json\n",
            "\n",
            "Processing tweet_subset_split_2.json, shape: (30000, 17)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-79-bedc56846792>:50: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  tweets_labels_train = tweets_labels_train.groupby('user_id').apply(lambda x: x.sample(n=min(5, len(x)), random_state=42)).reset_index(drop=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train (2835, 4)\n",
            "val (4000, 4)\n",
            "test (3746, 4)\n",
            "Processed and added data for tweet_subset_split_2.json\n",
            "\n",
            "Processing tweet_subset_split_3.json, shape: (30000, 17)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-79-bedc56846792>:50: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  tweets_labels_train = tweets_labels_train.groupby('user_id').apply(lambda x: x.sample(n=min(5, len(x)), random_state=42)).reset_index(drop=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train (2759, 4)\n",
            "val (4300, 4)\n",
            "test (4000, 4)\n",
            "Processed and added data for tweet_subset_split_3.json\n",
            "\n",
            "Saved train data to tweets_train.json\n",
            "Saved val data to tweets_val.json\n",
            "Saved test data to tweets_test.json\n",
            "Read tweets_train.json, total records: 8355\n",
            "Read tweets_val.json, total records: 11955\n",
            "Read tweets_test.json, total records: 12109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "train_file = 'tweets_train.json'\n",
        "val_file = 'tweets_val.json'\n",
        "test_file = 'tweets_test.json'\n",
        "\n",
        "# Read and count rows in each file\n",
        "for file_name, split_name in zip([train_file, val_file, test_file], ['Train', 'Validation', 'Test']):\n",
        "    try:\n",
        "        # Open the JSON file and load its content\n",
        "        with open(file_name, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Print the number of rows\n",
        "        print(f\"{split_name} file '{file_name}' contains {len(data)} rows.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"{split_name} file '{file_name}' not found.\")\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error reading {file_name}: Invalid JSON format.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r31AS-JcAxOE",
        "outputId": "5a17d228-d2c0-4024-e22b-b1b01a10b124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train file 'tweets_train.json' contains 8355 rows.\n",
            "Validation file 'tweets_val.json' contains 11955 rows.\n",
            "Test file 'tweets_test.json' contains 12109 rows.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "GpEeLNKtCNzE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "b3da57ba-4b4b-49f9-89ba-325280d338c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                user_id                                               text  \\\n",
              "0  u1459391889489072130  RT @BhaveshDagla: #जालोर जिले के बागोडा़ तहसील...   \n",
              "1            u221777661  RT @quinta: Pare proprio ci stiano dando dentr...   \n",
              "2            u221777661  RT @Kevin2600: These are what I think are must...   \n",
              "3            u221777661  RT @FPFabrizioPhD: Quello che è successo quest...   \n",
              "4            u221777661  RT @LatestAnonPress: BREAKING: #Anonymous inva...   \n",
              "\n",
              "  split  label  \n",
              "0  test  human  \n",
              "1  test  human  \n",
              "2  test  human  \n",
              "3  test  human  \n",
              "4  test  human  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a776a117-a1fa-4cce-8502-1c65a15f3c44\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>text</th>\n",
              "      <th>split</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>u1459391889489072130</td>\n",
              "      <td>RT @BhaveshDagla: #जालोर जिले के बागोडा़ तहसील...</td>\n",
              "      <td>test</td>\n",
              "      <td>human</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>u221777661</td>\n",
              "      <td>RT @quinta: Pare proprio ci stiano dando dentr...</td>\n",
              "      <td>test</td>\n",
              "      <td>human</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>u221777661</td>\n",
              "      <td>RT @Kevin2600: These are what I think are must...</td>\n",
              "      <td>test</td>\n",
              "      <td>human</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>u221777661</td>\n",
              "      <td>RT @FPFabrizioPhD: Quello che è successo quest...</td>\n",
              "      <td>test</td>\n",
              "      <td>human</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>u221777661</td>\n",
              "      <td>RT @LatestAnonPress: BREAKING: #Anonymous inva...</td>\n",
              "      <td>test</td>\n",
              "      <td>human</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a776a117-a1fa-4cce-8502-1c65a15f3c44')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a776a117-a1fa-4cce-8502-1c65a15f3c44 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a776a117-a1fa-4cce-8502-1c65a15f3c44');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f119ed0b-b415-4182-b03e-57b99bda32fb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f119ed0b-b415-4182-b03e-57b99bda32fb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f119ed0b-b415-4182-b03e-57b99bda32fb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 12110,\n  \"fields\": [\n    {\n      \"column\": \"user_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 352,\n        \"samples\": [\n          \"u2277152287\",\n          \"u1384039466990379015\",\n          \"u901326474\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12107,\n        \"samples\": [\n          \"RT @believeinscot: \\ud83d\\udfe9 A House of Lords report on the Union published last week has gone further than any before in recognising the possibili\\u2026\",\n          \"RT @plugimi: Doing lots of things at @transmediale 17, exhibitions and presentations Fri 3pm &amp; Sat 11.30am and a screening at @spektrumberl\\u2026\",\n          \"https://t.co/8Q2DkVAYqK\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"split\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"human\",\n          \"test\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"bot\",\n          \"human\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}