{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_file_path = \"Data/tweet_8.json.zip\"\n",
    "extraction_path = \"Data/Tweets\"\n",
    "\n",
    "# Extract the ZIP file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extraction_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmod\n",
    "await lmod.purge(force=True)\n",
    "await lmod.load('jdk/17.0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b04-14.hpc.usc.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Our First Spark Example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7033686c60>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from typing import List\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark= SparkSession.builder.appName(\"Our First Spark Example\").config(\"spark.executor.memory\", \"96g\").config(\"spark.driver.memory\", \"64g\").getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Read Data\n",
    "tweet_data = spark.read.option(\"multiline\", \"true\").json(\"Data/Tweets/tweet_1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts features of Average of URL'S,HASHTAGS,USER_MENTIONS AND SYMBOLS PRESENT IN TWEET\n",
    "subset = tweet_data.select(\"author_id\",\"entities\")\n",
    "subset = subset.withColumn(\"urls\", F.col(\"entities.urls\"))\\\n",
    "            .withColumn(\"hashtags\",F.col(\"entities.hashtags\"))\\\n",
    "            .withColumn(\"symbols\",F.col(\"entities.symbols\")) \\\n",
    "            .withColumn(\"user_mentions\",F.col(\"entities.user_mentions\"))\n",
    "subset = subset.drop(\"entities\")\n",
    "subset = subset.withColumn('urls', F.when(F.col('urls').isNull(), F.array()).otherwise(F.col('urls')))\\\n",
    "        .withColumn('hashtags', F.when(F.col('hashtags').isNull(), F.array()).otherwise(F.col('hashtags')))\\\n",
    "        .withColumn('symbols', F.when(F.col('symbols').isNull(), F.array()).otherwise(F.col('symbols')))\\\n",
    "        .withColumn('user_mentions', F.when(F.col('user_mentions').isNull(), F.array()).otherwise(F.col('user_mentions')))\n",
    "subset = subset.withColumn(\"num_urls\", F.size(F.col(\"urls\"))) \\\n",
    "       .withColumn(\"num_hashtags\", F.size(F.col(\"hashtags\"))) \\\n",
    "       .withColumn(\"num_symbols\", F.size(F.col(\"symbols\"))) \\\n",
    "       .withColumn(\"num_user_mentions\", F.size(F.col(\"user_mentions\")))\n",
    "\n",
    "# 2. Group by 'author_id' and calculate average counts for each entity type\n",
    "result_df = subset.groupBy(\"author_id\").agg(\n",
    "    F.avg(\"num_urls\").alias(\"avg_num_urls\"),\n",
    "    F.avg(\"num_hashtags\").alias(\"avg_num_hashtags\"),\n",
    "    F.avg(\"num_symbols\").alias(\"avg_num_symbols\"),\n",
    "    F.avg(\"num_user_mentions\").alias(\"avg_num_user_mentions\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most frequent language of the user \n",
    "lang_count_df = tweet_data.groupBy(\"author_id\", \"lang\").count()\n",
    "\n",
    "# 4. Use a window function to get the most frequent language for each user\n",
    "window_spec = Window.partitionBy(\"author_id\").orderBy(F.desc(\"count\"))\n",
    "\n",
    "# 5. Add a rank column to identify the most frequent language\n",
    "lang_ranked_df = lang_count_df.withColumn(\"rank\", F.rank().over(window_spec))\n",
    "\n",
    "# 6. Filter to keep only the most used language (rank = 1)\n",
    "most_used_lang_df = lang_ranked_df.filter(F.col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "#7. Join the language information with the averages dataframe\n",
    "final_df = result_df.join(most_used_lang_df.select(\"author_id\", \"lang\"), on=\"author_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweet_pattern = r\"^RT\\s+@\\w+:\"  # Matches 'RT @user:'\n",
    "mention_pattern = r\"@\\w+\"         # Matches user mentions '@username'\n",
    "url_pattern = r\"http\\S+|www.\\S+\"  # Matches URLs starting with http or www\n",
    "hashtag_pattern = r\"#\\w+\"         # Matches hashtags\n",
    "\n",
    "# 2. Clean the 'text' column by removing retweets, mentions, URLs, and hashtags\n",
    "df_cleaned = tweet_data.withColumn(\"clean_text\", F.col(\"text\")) \\\n",
    "    .withColumn(\"clean_text\", F.regexp_replace(F.col(\"clean_text\"), retweet_pattern, \"\")) \\\n",
    "    .withColumn(\"clean_text\", F.regexp_replace(F.col(\"clean_text\"), mention_pattern, \"\")) \\\n",
    "    .withColumn(\"clean_text\", F.regexp_replace(F.col(\"clean_text\"), url_pattern, \"\")) \\\n",
    "    .withColumn(\"clean_text\", F.regexp_replace(F.col(\"clean_text\"), hashtag_pattern, \"\")) \\\n",
    "    .withColumn(\"clean_text\", F.trim(F.col(\"clean_text\")))  # Trim whitespace from the result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.withColumn(\"clean_text_length\", F.length(F.col(\"clean_text\")))\n",
    "\n",
    "# 2. Group by 'author_id' and calculate the average length of the cleaned tweets\n",
    "avg_tweet_length_df = df_cleaned.groupBy(\"author_id\").agg(\n",
    "    F.avg(\"clean_text_length\").alias(\"avg_clean_text_length\")\n",
    ")\n",
    "\n",
    "final_combined_df = final_df.join(avg_tweet_length_df, on=\"author_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metric = tweet_data.withColumn(\"retweet_count\", F.coalesce(F.col(\"public_metrics.retweet_count\"), F.lit(0))) \\\n",
    "                      .withColumn(\"reply_count\", F.coalesce(F.col(\"public_metrics.reply_count\"), F.lit(0))) \\\n",
    "                      .withColumn(\"like_count\", F.coalesce(F.col(\"public_metrics.like_count\"), F.lit(0))) \\\n",
    "                      .withColumn(\"quote_count\", F.coalesce(F.col(\"public_metrics.quote_count\"), F.lit(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_metrics_df = df_metric.groupBy(\"author_id\").agg(\n",
    "    F.avg(\"retweet_count\").alias(\"avg_retweet_count\"),\n",
    "    F.avg(\"reply_count\").alias(\"avg_reply_count\"),\n",
    "    F.avg(\"like_count\").alias(\"avg_like_count\"),\n",
    "    F.avg(\"quote_count\").alias(\"avg_quote_count\")\n",
    ")\n",
    "\n",
    "# 3. Combine the new averages with the previous final_combined_df\n",
    "final_combined_df = final_combined_df.join(avg_metrics_df, on=\"author_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_combined_df.coalesce(1).write.parquet(\"Data/SparkOutput\",mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5937892,
     "sourceId": 9708350,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5937441,
     "sourceId": 9717515,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3.12.7 (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
